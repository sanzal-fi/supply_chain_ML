# Modelo de PredicciÃ³n de Demoras en Entregas

Este proyecto desarrolla un modelo de aprendizaje supervisado para predecir si una orden de compra serÃ¡ entregada a tiempo o con demora, utilizando algoritmos de clasificaciÃ³n y anÃ¡lisis de caracterÃ­sticas comerciales, logÃ­sticas y geogrÃ¡ficas.

## ğŸ¯ Objetivo

Construir una herramienta que permita anticiparse con precisiÃ³n a las demoras logÃ­sticas, de forma tal que la empresa pueda actuar proactivamente para mejorar su desempeÃ±o operativo y su nivel de servicio.

## ğŸ“Š Dataset

- **Archivo**: `DataCoSupplyChainDataset.csv`
- **Registros**: 180,519 Ã³rdenes
- **Variables**: 53 caracterÃ­sticas incluyendo informaciÃ³n de clientes, productos, Ã³rdenes y envÃ­os
- **Variable Objetivo**: `demora` (1 si hay demora, 0 si es a tiempo)

## ğŸ—ï¸ Estructura del Proyecto

```
trabajo_practico/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_analisis_exploratorio.ipynb    # AnÃ¡lisis exploratorio completo
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_preprocessing.py              # Preprocesamiento de datos
â”‚   â”œâ”€â”€ train_knn.py                       # Entrenamiento KNN
â”‚   â”œâ”€â”€ train_svm.py                       # Entrenamiento SVM
â”‚   â”œâ”€â”€ train_xgboost.py                   # Entrenamiento XGBoost
â”‚   â””â”€â”€ evaluate_models.py                 # EvaluaciÃ³n y comparaciÃ³n
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                         # Datos preprocesados
â”‚   â””â”€â”€ results/                           # Resultados y mÃ©tricas
â”œâ”€â”€ models/                                # Modelos entrenados
â””â”€â”€ requirements.txt                       # Dependencias
```

## ğŸš€ InstalaciÃ³n

1. **Clonar o descargar el proyecto**
2. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“‹ Dependencias

- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- xgboost
- jupyter
- joblib

## ğŸ”„ Flujo de Trabajo

### 1. AnÃ¡lisis Exploratorio de Datos (EDA)
**Archivo**: `notebooks/01_analisis_exploratorio.ipynb`

- Carga y exploraciÃ³n inicial del dataset
- CreaciÃ³n de variable objetivo `demora`
- AnÃ¡lisis de calidad de datos (nulos, duplicados)
- DetecciÃ³n y tratamiento de outliers
- AnÃ¡lisis univariado y bivariado
- IdentificaciÃ³n de variables relevantes

### 2. Preprocesamiento de Datos
**Archivo**: `scripts/data_preprocessing.py`

```bash
python scripts/data_preprocessing.py
```

**Funciones principales**:
- EliminaciÃ³n de columnas irrelevantes/sensibles
- Tratamiento de valores nulos y duplicados
- CodificaciÃ³n one-hot de variables categÃ³ricas
- EstandarizaciÃ³n de variables numÃ©ricas
- DivisiÃ³n estratificada train/test (80/20)

### 3. Entrenamiento de Modelos

#### KNN (K-Nearest Neighbors)
```bash
python scripts/train_knn.py
```

#### SVM (Support Vector Machine)
```bash
python scripts/train_svm.py
```

#### XGBoost (Extreme Gradient Boosting)
```bash
python scripts/train_xgboost.py
```

**CaracterÃ­sticas de entrenamiento**:
- BÃºsqueda de hiperparÃ¡metros con GridSearchCV
- ValidaciÃ³n cruzada estratificada (5-fold)
- OptimizaciÃ³n basada en F1-Score
- Guardado automÃ¡tico de modelos y parÃ¡metros

### 4. EvaluaciÃ³n y ComparaciÃ³n
**Archivo**: `scripts/evaluate_models.py`

```bash
python scripts/evaluate_models.py
```

**MÃ©tricas evaluadas**:
- Accuracy
- Precision
- Recall
- F1-Score
- AUC-ROC

**Visualizaciones generadas**:
- Matrices de confusiÃ³n
- Curvas ROC
- ComparaciÃ³n de mÃ©tricas
- Reportes detallados

## ğŸ“ˆ Resultados

Los resultados se guardan automÃ¡ticamente en `data/results/`:

- `model_comparison.csv`: Tabla comparativa de mÃ©tricas
- `confusion_matrices_comparison.png`: Matrices de confusiÃ³n
- `roc_curves_comparison.png`: Curvas ROC
- `metrics_comparison.png`: GrÃ¡fico de comparaciÃ³n
- `evaluation_report.txt`: Reporte final
- `xgboost_feature_importance.csv`: Importancia de caracterÃ­sticas (XGBoost)

## ğŸ¯ Algoritmos Utilizados

### 1. K-Nearest Neighbors (KNN)
- **HiperparÃ¡metros optimizados**: n_neighbors, weights, metric
- **Ventajas**: Simple, no paramÃ©trico, bueno para datos no lineales
- **Desventajas**: Computacionalmente costoso, sensible a outliers

### 2. Support Vector Machine (SVM)
- **HiperparÃ¡metros optimizados**: C, kernel, gamma
- **Ventajas**: Efectivo en espacios de alta dimensiÃ³n, robusto
- **Desventajas**: Lento con datasets grandes, sensible a escalado

### 3. XGBoost (Extreme Gradient Boosting)
- **HiperparÃ¡metros optimizados**: n_estimators, max_depth, learning_rate, subsample
- **Ventajas**: Alta precisiÃ³n, manejo de missing values, importancia de caracterÃ­sticas
- **Desventajas**: Puede sobreajustar, mÃ¡s complejo

## ğŸ”§ Uso de los Modelos

### Cargar un modelo entrenado:
```python
import joblib

# Cargar modelo
model = joblib.load('models/knn_model.pkl')

# Cargar scaler
scaler = joblib.load('data/processed/scaler.pkl')

# Hacer predicciÃ³n
prediction = model.predict(new_data)
```

### Preprocesar nuevos datos:
```python
from scripts.data_preprocessing import clean_data, encode_categorical, scale_features

# Aplicar mismo preprocesamiento que en entrenamiento
processed_data = clean_data(new_data)
processed_data = encode_categorical(processed_data)
processed_data, _ = scale_features(processed_data)
```

## ğŸ“Š InterpretaciÃ³n de Resultados

### MÃ©tricas Clave:
- **Accuracy**: ProporciÃ³n de predicciones correctas
- **Precision**: ProporciÃ³n de predicciones positivas que son correctas
- **Recall**: ProporciÃ³n de casos positivos detectados correctamente
- **F1-Score**: Media armÃ³nica entre precision y recall
- **AUC-ROC**: Ãrea bajo la curva ROC (capacidad de discriminaciÃ³n)

### Matriz de ConfusiÃ³n:
```
                 PredicciÃ³n
                 A Tiempo  Con Demora
Real A Tiempo     TN        FP
Real Con Demora   FN        TP
```

## ğŸ¯ Factores Clave Identificados

Basado en el anÃ¡lisis exploratorio, los factores mÃ¡s relevantes para predecir demoras incluyen:

- **Variables de tiempo**: DÃ­as de envÃ­o programados vs reales
- **Modo de envÃ­o**: Tipo de transporte utilizado
- **RegiÃ³n geogrÃ¡fica**: UbicaciÃ³n de destino
- **Volumen del pedido**: Cantidad y valor de productos
- **Segmento de cliente**: Tipo de cliente
- **CategorÃ­a de producto**: Tipo de productos

## ğŸ”„ Reproducibilidad

- **Semilla aleatoria**: `random_state=42` en todas las operaciones
- **DivisiÃ³n estratificada**: Mantiene proporciÃ³n de clases
- **ValidaciÃ³n cruzada**: 5-fold estratificada
- **Escalado consistente**: Mismo scaler para entrenamiento y prueba

## ğŸ“ Notas TÃ©cnicas

- **Balance de clases**: Se analiza la distribuciÃ³n de la variable objetivo
- **Tratamiento de outliers**: MÃ©todo IQR con capping
- **CodificaciÃ³n categÃ³rica**: One-hot encoding para todas las variables categÃ³ricas
- **Escalado**: StandardScaler para variables numÃ©ricas
- **ValidaciÃ³n**: SeparaciÃ³n estricta entre entrenamiento y prueba

## ğŸ¤ Contribuciones

Este proyecto fue desarrollado como parte de un trabajo prÃ¡ctico de Ciencia de Datos, implementando las mejores prÃ¡cticas en machine learning y anÃ¡lisis de datos.

## ğŸ“„ Licencia

Este proyecto es de uso acadÃ©mico y educativo.

---

**Desarrollado con â¤ï¸ para la comunidad de Ciencia de Datos**
# supply_chain_tp
